{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking BI data in HDFS via Spark SQL #\n",
    "\n",
    "## Background ##\n",
    "\n",
    "### Source data files ###\n",
    "\n",
    "* Business Index data ingestion process loads data from \"external\" source files into HDFS.\n",
    "* External sources are:\n",
    "\n",
    "> * Companies House (multiple CSV files)\n",
    "> * HMRC - PAYE (CSV - currently extracted from ONS IDBR)\n",
    "> * HMRC - VAT (CSV - currently extracted from ONS IDBR)\n",
    "\n",
    "* We also load a file of \"links\" i.e. triples of Companies House | VAT | PAYE references that have been matched via the data science pre-processing stage.\n",
    "* Each field in the triple may be empty, and there may be multiple VAT or PAYE references in one record.\n",
    "* The \"links\" file (also described as \"legal entities\") is in JSON format.\n",
    "\n",
    "### BI intermediate Parquet files ###\n",
    "\n",
    "* The BI data ingestion process has to perform multiple processing steps e.g. generating or identifying UBRN, matching link references to corresponding CH/VAT/PAYE records, constructing final Business Index record etc.\n",
    "* These steps are described in the README documentation for the BI project in Github.\n",
    "* We store intermediate data for each step in HDFS files in Parquet format, which is a compressed columnar data format that allows greater efficiency when querying and processing the data e.g. in Apache Spark.\n",
    "* This allows us to explore the intermediate data e.g. to look for bugs, check data contents etc.\n",
    "* This Jupyter notebook shows how you can use Spark SQL to do this.\n",
    "\n",
    "### Load BI source data files in Parquet ###\n",
    "\n",
    "* The first thing we do with each of the external source data files described above (CH, PAYE, VAT) is to load it into Parquet, to make it easier and quicker to process later on.\n",
    "* At this point we do not apply any extra transformations to the data, so the Parquet file should have the same record structure as the corresponding source data file.\n",
    "* The links data is modified during the initial load process to add a UBRN (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation ##\n",
    "\n",
    "* The Jupyter notebooks installation on the ONS  Cloudera cluster provides a default SparkContext object,  which is available as \"sc\".\n",
    "* We also need a SQLContext object in order to use  Spark SQL  operations here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS file locations ##\n",
    "\n",
    "* Check the README docs in Github for more information on file locations.\n",
    "* Our intermediate working data is stored under the \"WORKINGDATA\" directory.\n",
    "* The following code assumes we are using the default file names as specified in the BI application configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HDFS working directory roots \n",
    "### CHECK THE BASE DIRECTORY MATCHES YOUR INSTALLATION ###\n",
    "baseDir = \"./ons.gov/businessIndex/dev\"\n",
    "wDir = \"{0}/WORKINGDATA\".format(baseDir)\n",
    "\n",
    "# Source data files loaded into Parquet\n",
    "chFile = \"{0}/CH.parquet\".format(wDir)\n",
    "vatFile = \"{0}/VAT.parquet\".format(wDir)\n",
    "payeFile = \"{0}/PAYE.parquet\".format(wDir)\n",
    "linksFile = \"{0}/LINKS_Output.parquet\".format(wDir)\n",
    "\n",
    "# Generated Business Index data file (records will be copied to ElasticSearch)\n",
    "biFile = \"{0}/BI_Output.parquet\".format(wDir)\n",
    "\n",
    "# Previous links data (used when we have to apply \"month 2+\" logic to UBRN)\n",
    "prevDir =  \"{0}/PREVIOUS\".format(baseDir)\n",
    "prevFile = \"{0}/LINKS_Output.parquet\".format(prevDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links data in Parquet ##\n",
    "\n",
    "* The Links file is pre-processed to allocate each record a UBRN.\n",
    "* The UBRN allocation rules are evolving, but this will involve comparing the latest file's data with the links loaded for previous months.\n",
    "* The Parquet file contains the UBRN allocated via this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the file into Spark\n",
    "linksDf = sqlContext.read.parquet(linksFile)\n",
    "\n",
    "# Count the records (forces data to be materialised)\n",
    "print(\"LINKS contains {0} records.\".format(str(linksDf.count())))\n",
    "\n",
    "# Make the file available as a Spark SQL table\n",
    "linksDf.registerTempTable(\"links\")\n",
    "\n",
    "# Display the record structure\n",
    "linksDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display the first few records\n",
    "linksDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of SQL query on links data ###\n",
    "\n",
    "* Remember that PAYE and VAT references are actually arrays in the links record.\n",
    "* You can specify that you want the first VAT reference in the array as \"VAT[0]\", for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lndata = sqlContext.sql(\"SELECT * FROM links WHERE VAT[0] = 220062373000 LIMIT 5\")\n",
    "lndata.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Companies House data in Parquet ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chDf = sqlContext.read.parquet(chFile)\n",
    "\n",
    "print(\"CH contains {0} records.\".format(str(chDf.count())))\n",
    "\n",
    "chDf.registerTempTable(\"ch\")\n",
    "\n",
    "chDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of SQL query on CH data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sqlContext.sql(\"SELECT COUNT(*) AS bad_recs FROM ch WHERE CompanyNumber = 'CompanyNumber'\").limit(1)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAT data in Parquet ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vatDf = sqlContext.read.parquet(vatFile)\n",
    "\n",
    "print(\"VAT contains {0} records.\".format(str(vatDf.count())))\n",
    "\n",
    "vatDf.registerTempTable(\"vat\")\n",
    "\n",
    "vatDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example  of SQL query on VAT data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vatData = sqlContext.sql(\"SELECT * FROM vat WHERE vatref = 656091134000\").limit(1)\n",
    "vatData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAYE data in Parquet ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "payeDf = sqlContext.read.parquet(payeFile)\n",
    "print(\"PAYE contains {0} records.\".format(str(payeDf.count())))\n",
    "payeDf.registerTempTable(\"paye\")\n",
    "payeDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of SQL query on PAYE data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "payeData = sqlContext.sql(\"SELECT payeref, name1 FROM paye WHERE LENGTH(payeref) = 4\").limit(5)\n",
    "paye_recs = payeData.collect()\n",
    "\n",
    "for pr in paye_recs:\n",
    "    print(\"Ref: {0}  Name: {1}\".format(pr['payeref'],pr['name1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Index data in Parquet ##\n",
    "\n",
    "* The BI data file is constructed by the data ingestion process.\n",
    "* We join the incoming links to the corresponding CH/PAYE/VAT data.\n",
    "* Then we build a record with the required fields from each source.\n",
    "* This BI record is written to a Parquet file.\n",
    "* The final step in BI data ingestion will simply copy these records to ElasticSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "biDf = sqlContext.read.parquet(biFile)\n",
    "\n",
    "print(\"BI contains {0} records.\".format(str(biDf.count())))\n",
    "\n",
    "biDf.registerTempTable(\"bi\")\n",
    "\n",
    "biDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of SQL query on BI data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sqlContext.sql(\"SELECT BusinessName, PostCode, IndustryCode, LegalStatus, EmploymentBands, PayeRefs FROM bi WHERE BusinessName = 'MOTA-TEST'\").limit(1)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
