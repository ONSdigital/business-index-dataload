
#
# Business Index: Dataload Configuration
# ======================================
#
# Changing configuration settings at runtime:
#
# This config file provides a vanilla default value, as shown below.
#
# It also allows the property to be overridden via a specified env variable if provided.
#
# The environment variable is named like this:
#
#   e.g. config property:        bi-dataload.es.index
#        corresponding variable: BI_DATALOAD_ES_INDEX
#
# So you can override config values here at runtime either via Java driver options
# or via environment variables.
#
# (a) via command-line Java driver options (e.g. in Oozie):
#
# spark-submit --class com.example.Sparky \
#              --master local[*] \
#              --driver-java-options "-Dbi-dataload.es.index=my_index_name" \
# target/scala-2.11/spark-dummy_2.11-1.0.jar
#
# (b) via a corresponding environment variable such as BI_DATALOAD_ES_INDEX.
#
# See the config parameters below for the corresponding environment variable names.
#
# Priority of config values:
#
# 1.  If value is provided via Java driver option, then this value will be used.
# 2.  Else if value is provided via env variable, then this value will be used.
# 3.  Else if no env or driver value exists, then value from config file will be used.
#

bi-dataload {

  # HMRC and CH files are regarded as "external" data that we may be
  # reading from a shared directory outside the BI app tree.
  # Proposed structure is:
  #
  # /external
  #     |
  #     +-- hmrc
  #     |     +--- paye
  #     |     +--- vat
  #     +-- companiesHouse

  ext-data {
    env = "dev"
    env = ${?BI_DATALOAD_EXT_DATA_ENV}
    dir = "external"
    dir = ${?BI_DATALOAD_EXT_DATA_DIR}
    ch-dir = "companiesHouse"
    ch-dir = ${?BI_DATALOAD_EXT_DATA_CH_DIR}
    vat-dir = "hmrc/vat"
    vat-dir = ${?BI_DATALOAD_EXT_DATA_VAT_DIR}
    paye-dir = "hmrc/paye"
    paye-dir = ${?BI_DATALOAD_EXT_DATA_PAYE_DIR}

    # Files: we will read ALL CSV files in specified folder by default.
    paye = "*.csv"
    paye = ${?BI_DATALOAD_EXT_DATA_PAYE}
    vat = "*.csv"
    vat = ${?BI_DATALOAD_EXT_DATA_VAT}
    ch = "*.csv"
    ch = ${?BI_DATALOAD_EXT_DATA_CH}
  }

  # Links JSON file is really source data too, but it comes from ONS, so
  # we need to read this from a different directory (see Apps below).
  # We now (May 2017) have to look up TCN code (in PAYE) to find SIC.
  # The lookup CSV file will be stored here.

  ons-data {
    dir = "ons.gov/businessIndex"

    links {
      parquet = "legal_units.parquet"
      parquet = ${?BI_DATALOAD_LINKS_DATA_PARQUET}
    }

    lookups {
      dir = "lookups"
      dir = ${?BI_DATALOAD_LOOKUPS_DIR}
      tcn-to-sic = "tcn-to-sic-mapping.csv"
      tcn-to-sic = ${?BI_DATALOAD_LOOKUPS_TCN_TO_SIC}
    }
  }

  # App data is created within the BI dataload application e.g. Parquet files.
  # This includes the previous Links data that is needed for subsequent runs.
  #
  # We copy the latest Links with UBRNS to PREVIOUS (see below) ready for next run.
  # We also write a file of the same Links under a timestamped folder.  Tihs means we always have a
  # copy of Links for previous runs, not just the most recent.
  #
  # There is also a requirement for an extra "env" directory layer i.e. dev/test/beta.
  #
  # Links and App data structure (assume env = dev):
  #
  # /ons.gov/businessIndex
  #              |
  #              +-- links:  contains Links JSON file (see above)
  #              |
  #              +-- lookups:  contains CSV lookup data (TCN to SIC)
  #              |
  #              +-- dev: or test or beta
  #                   |
  #                   +-- WORKINGDATA: contains our Parquet files (over-written in each run)
  #                   |
  #                   +-- PREVIOUS: contains Parquet file of Links with UBRNS from prev run
  #                          |
  #                          +-- 201703131145: contains Links with UBRNS written on given date/time.
  app-data {
    # data directories
    env = "dev"
    env = ${?BI_DATALOAD_APP_DATA_ENV}
    dir = "ons.gov/businessIndex"
    dir = ${?BI_DATALOAD_APP_DATA_DIR}
    work = "WORKINGDATA"
    work = ${?BI_DATALOAD_APP_DATA_WORK}
    prev = "PREVIOUS"
    prev = ${?BI_DATALOAD_APP_DATA_PREV}
    extract = "EXTRACT"
    extract = ${?BI_DATALOAD_APP_DATA_EXTRACT}
    # Parquet files
    paye = "PAYE.parquet"
    paye = ${?BI_DATALOAD_APP_DATA_PAYE}
    vat = "VAT.parquet"
    vat = ${?BI_DATALOAD_APP_DATA_VAT}
    ch = "CH.parquet"
    ch = ${?BI_DATALOAD_APP_DATA_CH}
    links = "LINKS_Output.parquet"
    links = ${?BI_DATALOAD_APP_DATA_LINKS}
    bi = "BI_Output.parquet"
    bi = ${?BI_DATALOAD_APP_DATA_BI}
    tcn = "TCN_TO_SIC_LOOKUP.parquet"
    tcn = ${?BI_DATALOAD_APP_DATA_TCN}
  }

  # ElasticSearch config

  es {
    nodes = "localhost"
    nodes = ${?BI_DATALOAD_ES_NODES}
    port = 9200
    port = ${?BI_DATALOAD_ES_PORT}
    es-user = "username"
    es-user = ${?BI_DATALOAD_ES_USER}
    es-pass = "password"
    es-pass = ${?BI_DATALOAD_ES_PASS}
    index = "bi-dev"
    index = ${?BI_DATALOAD_ES_INDEX}
    index-type = "business"
    index-type = ${?BI_DATALOAD_ES_INDEX_TYPE}
    # We expect the real index to exist already as it needs extra config that cannot be done here.
    # But using autocreate allows us to test with arbitrary index names.
    autocreate = "true"
    autocreate = ${?BI_DATALOAD_ES_AUTOCREATE}
    # WAN-only not used currently but may be needed (cf. Address Index)
    wan-only = "true"
    wan-only = ${?BI_DATALOAD_ES_WAN_ONLY}
    # Output directory for the parquet file of the Business Index to ElasticSearch output
    parquet-dir="/user/bi-dev-ci/businessIndex/ESOutput"
    parquet-dir=${?BI_DATALOAD_ES_PARQUET_DIR}
  }

  # Any default params we might want to provide for Spark
  spark {
    app-name = "business-indexes-dataload"
    app-name = ${?BI_DATALOAD_SPARK_APP_NAME}
    serializer = "org.apache.spark.serializer.KryoSerializer"
    serializer = ${?BI_DATALOAD_SPARK_SERIALIZER}
  }

}