

# One approach is to provide a vanilla default value
# then allow it to be over-ridden by a value provided:
#
# (a) at runtime via command-line Java driver options (e.g. in Oozie):
#
# spark-submit --class com.example.Sparky \
#              --master local[*] \
#              --driver-java-options "-Dbi-dataload.es.index=my_index_name" \
# target/scala-2.11/spark-dummy_2.11-1.0.jar
#
# or (b) as an environment variable such as BI_DATALOAD_ES_INDEX for a property
# called bi-dataload.es.index, for example.
#

bi-dataload {

  es {
    nodes = "localhost"
    nodes = ${?BI_DATALOAD_ES_NODES}
    port = 9200
    port = ${?BI_DATALOAD_ES_PORT}
    es-user = "username"
    es-user = ${?BI_DATALOAD_ES_USER}
    es-pass = "password"
    es-pass = ${?BI_DATALOAD_ES_PASS}
    index = "bi-dev"
    index = ${?BI_DATALOAD_ES_INDEX}
    index-type = "business"
    index-type = ${?BI_DATALOAD_ES_INDEX_TYPE}
    autocreate = "true"
    autocreate = ${?BI_DATALOAD_ES_AUTOCREATE}
    wan-only = "true"
    wan-only = ${?BI_DATALOAD_ES_WAN_ONLY}
  }

  # HMRC and CH files are regarded as "external" data that we may be
  # reading from a shared directory outside the BI app tree.
  # Proposed structure is:
  #
  # /external
  #     |
  #     +-- HMRC
  #     |     +--- PAYE
  #     |     +--- VAT
  #     +-- CompaniesHouse

  ext-data {
    dir = "./data/external"
    dir = ${?BI_DATALOAD_EXT_DATA_DIR}
    ch-dir = "CompaniesHouse"
    ch-dir = ${?BI_DATALOAD_EXT_DATA_CH_DIR}
    vat-dir = "HMRC/VAT"
    vat-dir = ${?BI_DATALOAD_EXT_DATA_VAT_DIR}
    paye-dir = "HMRC/PAYE"
    paye-dir = ${?BI_DATALOAD_EXT_DATA_PAYE_DIR}

    # Files: we will read ALL CSV files in specified folder by default.
    paye = "*.csv"
    paye = ${?BI_DATALOAD_EXT_DATA_PAYE}
    vat = "*.csv"
    vat = ${?BI_DATALOAD_EXT_DATA_VAT}
    ch = "*.csv"
    ch = ${?BI_DATALOAD_EXT_DATA_CH}
  }

  # Links JSON file is really source data too, but apparently
  # we need to read this from a different directory (see Apps below).

  links-data {
    dir = "./data/ons.gov/businessIndex/Links"
    dir = ${?BI_DATALOAD_LINKS_DATA_DIR}
    json = "links.json"
    json = ${?BI_DATALOAD_LINKS_DATA_JSON}
  }

  # App data is created within the BI dataload application e.g. Parquet files.
  # This includes the previous Links data that is needed for subsequent runs.
  #
  # We copy the latest Links with UBRNS to PREVIOUS (see below) ready for next run.
  # We also write a file of the same Links under a timestamped folder.  Tihs means we always have a
  # copy of Links for previous runs, not just the most recent.
  #
  # There is also a requirement for an extra "env" directory layer i.e. dev/test/beta.
  #
  # Links and App data structure (assume env = dev):
  #
  # /ons.gov/businessIndex
  #              |
  #              +-- Links:  contains Links JSON file (see above)
  #              |
  #              +-- dev: or test or beta
  #                   |
  #                   +-- WORKINGDATA: contains our Parquet files (over-written in each run)
  #                   |
  #                   +-- PREVIOUS: contains Parquet file of Links with UBRNS from prev run
  #                          |
  #                          +-- 201703131145: contains Links with UBRNS written on given date/time.
  app-data {
    # data drectories
    env = "dev"
    env = ${?BI_DATALOAD_APP_DATA_ENV}
    dir = "./data/ons.gov/businessIndex"
    dir = ${?BI_DATALOAD_APP_DATA_DIR}
    work = "WORKINGDATA"
    work = ${?BI_DATALOAD_APP_DATA_WORK}
    prev = "PREVIOUS"
    # Parquet files
    prev = ${?BI_DATALOAD_APP_DATA_PREV}
    paye = "PAYE.parquet"
    paye = ${?BI_DATALOAD_APP_DATA_PAYE}
    vat = "VAT.parquet"
    vat = ${?BI_DATALOAD_APP_DATA_VAT}
    ch = "CH.parquet"
    ch = ${?BI_DATALOAD_APP_DATA_CH}
    links = "LINKS_Output.parquet"
    links = ${?BI_DATALOAD_APP_DATA_LINKS}
    bi = "BI_Output.parquet"
    bi = ${?BI_DATALOAD_APP_DATA_BI}
  }

  # Any default params we might want to provide for Spark
  spark {
    app-name = "business-indexes-dataload"
    app-name = ${?BI_DATALOAD_SPARK_APP_NAME}
    serializer = "org.apache.spark.serializer.KryoSerializer"
    serializer = ${?BI_DATALOAD_SPARK_SERIALIZER}
  }

}